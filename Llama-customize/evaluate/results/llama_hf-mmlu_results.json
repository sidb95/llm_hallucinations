{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-anatomy": {
      "acc": 0.45925925925925926,
      "acc_stderr": 0.04304979692464243,
      "acc_norm": 0.45925925925925926,
      "acc_norm_stderr": 0.04304979692464243
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5723684210526315,
      "acc_stderr": 0.04026097083296564,
      "acc_norm": 0.5723684210526315,
      "acc_norm_stderr": 0.04026097083296564
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.53,
      "acc_stderr": 0.05016135580465919,
      "acc_norm": 0.53,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5849056603773585,
      "acc_stderr": 0.03032594578928611,
      "acc_norm": 0.5849056603773585,
      "acc_norm_stderr": 0.03032594578928611
    },
    "hendrycksTest-college_biology": {
      "acc": 0.5833333333333334,
      "acc_stderr": 0.04122728707651282,
      "acc_norm": 0.5833333333333334,
      "acc_norm_stderr": 0.04122728707651282
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.47,
      "acc_stderr": 0.05016135580465919,
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.27,
      "acc_stderr": 0.0446196043338474,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.0446196043338474
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4624277456647399,
      "acc_stderr": 0.0380168510452446,
      "acc_norm": 0.4624277456647399,
      "acc_norm_stderr": 0.0380168510452446
    },
    "hendrycksTest-college_physics": {
      "acc": 0.3137254901960784,
      "acc_stderr": 0.04617034827006717,
      "acc_norm": 0.3137254901960784,
      "acc_norm_stderr": 0.04617034827006717
    },
    "hendrycksTest-computer_security": {
      "acc": 0.69,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.69,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.39574468085106385,
      "acc_stderr": 0.031967586978353627,
      "acc_norm": 0.39574468085106385,
      "acc_norm_stderr": 0.031967586978353627
    },
    "hendrycksTest-econometrics": {
      "acc": 0.32456140350877194,
      "acc_stderr": 0.044045561573747664,
      "acc_norm": 0.32456140350877194,
      "acc_norm_stderr": 0.044045561573747664
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.5103448275862069,
      "acc_stderr": 0.04165774775728762,
      "acc_norm": 0.5103448275862069,
      "acc_norm_stderr": 0.04165774775728762
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.3412698412698413,
      "acc_stderr": 0.024419234966819064,
      "acc_norm": 0.3412698412698413,
      "acc_norm_stderr": 0.024419234966819064
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.30158730158730157,
      "acc_stderr": 0.04104947269903394,
      "acc_norm": 0.30158730158730157,
      "acc_norm_stderr": 0.04104947269903394
    },
    "hendrycksTest-global_facts": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.6483870967741936,
      "acc_stderr": 0.027162537826948458,
      "acc_norm": 0.6483870967741936,
      "acc_norm_stderr": 0.027162537826948458
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.45320197044334976,
      "acc_stderr": 0.03502544650845872,
      "acc_norm": 0.45320197044334976,
      "acc_norm_stderr": 0.03502544650845872
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.59,
      "acc_stderr": 0.04943110704237102,
      "acc_norm": 0.59,
      "acc_norm_stderr": 0.04943110704237102
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6787878787878788,
      "acc_stderr": 0.03646204963253811,
      "acc_norm": 0.6787878787878788,
      "acc_norm_stderr": 0.03646204963253811
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.7070707070707071,
      "acc_stderr": 0.03242497958178816,
      "acc_norm": 0.7070707070707071,
      "acc_norm_stderr": 0.03242497958178816
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.7927461139896373,
      "acc_stderr": 0.02925282329180363,
      "acc_norm": 0.7927461139896373,
      "acc_norm_stderr": 0.02925282329180363
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.49743589743589745,
      "acc_stderr": 0.025350672979412202,
      "acc_norm": 0.49743589743589745,
      "acc_norm_stderr": 0.025350672979412202
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.3074074074074074,
      "acc_stderr": 0.028133252578815642,
      "acc_norm": 0.3074074074074074,
      "acc_norm_stderr": 0.028133252578815642
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.5294117647058824,
      "acc_stderr": 0.03242225027115007,
      "acc_norm": 0.5294117647058824,
      "acc_norm_stderr": 0.03242225027115007
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.33112582781456956,
      "acc_stderr": 0.038425817186598696,
      "acc_norm": 0.33112582781456956,
      "acc_norm_stderr": 0.038425817186598696
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7431192660550459,
      "acc_stderr": 0.01873249292834247,
      "acc_norm": 0.7431192660550459,
      "acc_norm_stderr": 0.01873249292834247
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.39351851851851855,
      "acc_stderr": 0.03331747876370312,
      "acc_norm": 0.39351851851851855,
      "acc_norm_stderr": 0.03331747876370312
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.75,
      "acc_stderr": 0.03039153369274154,
      "acc_norm": 0.75,
      "acc_norm_stderr": 0.03039153369274154
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.7172995780590717,
      "acc_stderr": 0.029312814153955924,
      "acc_norm": 0.7172995780590717,
      "acc_norm_stderr": 0.029312814153955924
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6322869955156951,
      "acc_stderr": 0.03236198350928275,
      "acc_norm": 0.6322869955156951,
      "acc_norm_stderr": 0.03236198350928275
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.6183206106870229,
      "acc_stderr": 0.042607351576445594,
      "acc_norm": 0.6183206106870229,
      "acc_norm_stderr": 0.042607351576445594
    },
    "hendrycksTest-international_law": {
      "acc": 0.7768595041322314,
      "acc_stderr": 0.03800754475228732,
      "acc_norm": 0.7768595041322314,
      "acc_norm_stderr": 0.03800754475228732
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.7037037037037037,
      "acc_stderr": 0.044143436668549335,
      "acc_norm": 0.7037037037037037,
      "acc_norm_stderr": 0.044143436668549335
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.6503067484662577,
      "acc_stderr": 0.037466683254700206,
      "acc_norm": 0.6503067484662577,
      "acc_norm_stderr": 0.037466683254700206
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04547960999764376,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04547960999764376
    },
    "hendrycksTest-management": {
      "acc": 0.7378640776699029,
      "acc_stderr": 0.04354631077260595,
      "acc_norm": 0.7378640776699029,
      "acc_norm_stderr": 0.04354631077260595
    },
    "hendrycksTest-marketing": {
      "acc": 0.7905982905982906,
      "acc_stderr": 0.026655699653922726,
      "acc_norm": 0.7905982905982906,
      "acc_norm_stderr": 0.026655699653922726
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.56,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.7496807151979565,
      "acc_stderr": 0.015491088951494574,
      "acc_norm": 0.7496807151979565,
      "acc_norm_stderr": 0.015491088951494574
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.6040462427745664,
      "acc_stderr": 0.02632981334194624,
      "acc_norm": 0.6040462427745664,
      "acc_norm_stderr": 0.02632981334194624
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.30837988826815643,
      "acc_stderr": 0.01544571691099887,
      "acc_norm": 0.30837988826815643,
      "acc_norm_stderr": 0.01544571691099887
    },
    "hendrycksTest-nutrition": {
      "acc": 0.6143790849673203,
      "acc_stderr": 0.02787074527829027,
      "acc_norm": 0.6143790849673203,
      "acc_norm_stderr": 0.02787074527829027
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5980707395498392,
      "acc_stderr": 0.027846476005930477,
      "acc_norm": 0.5980707395498392,
      "acc_norm_stderr": 0.027846476005930477
    },
    "hendrycksTest-prehistory": {
      "acc": 0.5987654320987654,
      "acc_stderr": 0.027272582849839792,
      "acc_norm": 0.5987654320987654,
      "acc_norm_stderr": 0.027272582849839792
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.37943262411347517,
      "acc_stderr": 0.028947338851614105,
      "acc_norm": 0.37943262411347517,
      "acc_norm_stderr": 0.028947338851614105
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3917861799217731,
      "acc_stderr": 0.012467564418145121,
      "acc_norm": 0.3917861799217731,
      "acc_norm_stderr": 0.012467564418145121
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.49264705882352944,
      "acc_stderr": 0.030369552523902173,
      "acc_norm": 0.49264705882352944,
      "acc_norm_stderr": 0.030369552523902173
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.5359477124183006,
      "acc_stderr": 0.020175488765484043,
      "acc_norm": 0.5359477124183006,
      "acc_norm_stderr": 0.020175488765484043
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6636363636363637,
      "acc_stderr": 0.04525393596302505,
      "acc_norm": 0.6636363636363637,
      "acc_norm_stderr": 0.04525393596302505
    },
    "hendrycksTest-security_studies": {
      "acc": 0.6326530612244898,
      "acc_stderr": 0.030862144921087558,
      "acc_norm": 0.6326530612244898,
      "acc_norm_stderr": 0.030862144921087558
    },
    "hendrycksTest-sociology": {
      "acc": 0.746268656716418,
      "acc_stderr": 0.03076944496729602,
      "acc_norm": 0.746268656716418,
      "acc_norm_stderr": 0.03076944496729602
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.81,
      "acc_stderr": 0.03942772444036625,
      "acc_norm": 0.81,
      "acc_norm_stderr": 0.03942772444036625
    },
    "hendrycksTest-virology": {
      "acc": 0.4879518072289157,
      "acc_stderr": 0.03891364495835821,
      "acc_norm": 0.4879518072289157,
      "acc_norm_stderr": 0.03891364495835821
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7368421052631579,
      "acc_stderr": 0.03377310252209205,
      "acc_norm": 0.7368421052631579,
      "acc_norm_stderr": 0.03377310252209205
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=meta-llama/Llama-2-13b-chat-hf,tokenizer=meta-llama/Llama-2-13b-chat-hf,use_accelerate=True,device_map_option=auto",
    "num_fewshot": 5,
    "batch_size": "auto",
    "batch_sizes": [
      2
    ],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}