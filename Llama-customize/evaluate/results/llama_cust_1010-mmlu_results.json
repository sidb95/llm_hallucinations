{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4962962962962963,
      "acc_stderr": 0.04319223625811331,
      "acc_norm": 0.4962962962962963,
      "acc_norm_stderr": 0.04319223625811331
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5394736842105263,
      "acc_stderr": 0.04056242252249033,
      "acc_norm": 0.5394736842105263,
      "acc_norm_stderr": 0.04056242252249033
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.48,
      "acc_stderr": 0.050211673156867795,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5132075471698113,
      "acc_stderr": 0.030762134874500476,
      "acc_norm": 0.5132075471698113,
      "acc_norm_stderr": 0.030762134874500476
    },
    "hendrycksTest-college_biology": {
      "acc": 0.5694444444444444,
      "acc_stderr": 0.04140685639111503,
      "acc_norm": 0.5694444444444444,
      "acc_norm_stderr": 0.04140685639111503
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620332,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620332
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4277456647398844,
      "acc_stderr": 0.03772446857518027,
      "acc_norm": 0.4277456647398844,
      "acc_norm_stderr": 0.03772446857518027
    },
    "hendrycksTest-college_physics": {
      "acc": 0.35294117647058826,
      "acc_stderr": 0.04755129616062946,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.04755129616062946
    },
    "hendrycksTest-computer_security": {
      "acc": 0.7,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.7,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4127659574468085,
      "acc_stderr": 0.03218471141400352,
      "acc_norm": 0.4127659574468085,
      "acc_norm_stderr": 0.03218471141400352
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2982456140350877,
      "acc_stderr": 0.04303684033537315,
      "acc_norm": 0.2982456140350877,
      "acc_norm_stderr": 0.04303684033537315
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.47586206896551725,
      "acc_stderr": 0.041618085035015295,
      "acc_norm": 0.47586206896551725,
      "acc_norm_stderr": 0.041618085035015295
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.328042328042328,
      "acc_stderr": 0.0241804971643769,
      "acc_norm": 0.328042328042328,
      "acc_norm_stderr": 0.0241804971643769
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.29365079365079366,
      "acc_stderr": 0.04073524322147125,
      "acc_norm": 0.29365079365079366,
      "acc_norm_stderr": 0.04073524322147125
    },
    "hendrycksTest-global_facts": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252605,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252605
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.6129032258064516,
      "acc_stderr": 0.02770935967503249,
      "acc_norm": 0.6129032258064516,
      "acc_norm_stderr": 0.02770935967503249
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.39901477832512317,
      "acc_stderr": 0.03445487686264716,
      "acc_norm": 0.39901477832512317,
      "acc_norm_stderr": 0.03445487686264716
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.6,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6606060606060606,
      "acc_stderr": 0.03697442205031596,
      "acc_norm": 0.6606060606060606,
      "acc_norm_stderr": 0.03697442205031596
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.6767676767676768,
      "acc_stderr": 0.033322999210706444,
      "acc_norm": 0.6767676767676768,
      "acc_norm_stderr": 0.033322999210706444
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.7098445595854922,
      "acc_stderr": 0.032752644677915166,
      "acc_norm": 0.7098445595854922,
      "acc_norm_stderr": 0.032752644677915166
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4666666666666667,
      "acc_stderr": 0.025294608023986472,
      "acc_norm": 0.4666666666666667,
      "acc_norm_stderr": 0.025294608023986472
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2962962962962963,
      "acc_stderr": 0.027840811495871923,
      "acc_norm": 0.2962962962962963,
      "acc_norm_stderr": 0.027840811495871923
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4957983193277311,
      "acc_stderr": 0.03247734334448111,
      "acc_norm": 0.4957983193277311,
      "acc_norm_stderr": 0.03247734334448111
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.33112582781456956,
      "acc_stderr": 0.038425817186598696,
      "acc_norm": 0.33112582781456956,
      "acc_norm_stderr": 0.038425817186598696
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7174311926605504,
      "acc_stderr": 0.019304243497707152,
      "acc_norm": 0.7174311926605504,
      "acc_norm_stderr": 0.019304243497707152
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.36574074074074076,
      "acc_stderr": 0.03284738857647207,
      "acc_norm": 0.36574074074074076,
      "acc_norm_stderr": 0.03284738857647207
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.7009803921568627,
      "acc_stderr": 0.03213325717373617,
      "acc_norm": 0.7009803921568627,
      "acc_norm_stderr": 0.03213325717373617
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6751054852320675,
      "acc_stderr": 0.030486039389105307,
      "acc_norm": 0.6751054852320675,
      "acc_norm_stderr": 0.030486039389105307
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6591928251121076,
      "acc_stderr": 0.0318114974705536,
      "acc_norm": 0.6591928251121076,
      "acc_norm_stderr": 0.0318114974705536
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5801526717557252,
      "acc_stderr": 0.043285772152629715,
      "acc_norm": 0.5801526717557252,
      "acc_norm_stderr": 0.043285772152629715
    },
    "hendrycksTest-international_law": {
      "acc": 0.743801652892562,
      "acc_stderr": 0.03984979653302872,
      "acc_norm": 0.743801652892562,
      "acc_norm_stderr": 0.03984979653302872
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.6851851851851852,
      "acc_stderr": 0.04489931073591312,
      "acc_norm": 0.6851851851851852,
      "acc_norm_stderr": 0.04489931073591312
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.6012269938650306,
      "acc_stderr": 0.038470214204560246,
      "acc_norm": 0.6012269938650306,
      "acc_norm_stderr": 0.038470214204560246
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04547960999764376,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04547960999764376
    },
    "hendrycksTest-management": {
      "acc": 0.6990291262135923,
      "acc_stderr": 0.0454160944650395,
      "acc_norm": 0.6990291262135923,
      "acc_norm_stderr": 0.0454160944650395
    },
    "hendrycksTest-marketing": {
      "acc": 0.7521367521367521,
      "acc_stderr": 0.028286324075564383,
      "acc_norm": 0.7521367521367521,
      "acc_norm_stderr": 0.028286324075564383
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.58,
      "acc_stderr": 0.04960449637488583,
      "acc_norm": 0.58,
      "acc_norm_stderr": 0.04960449637488583
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.7369093231162197,
      "acc_stderr": 0.015745497169049053,
      "acc_norm": 0.7369093231162197,
      "acc_norm_stderr": 0.015745497169049053
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5809248554913294,
      "acc_stderr": 0.026564178111422622,
      "acc_norm": 0.5809248554913294,
      "acc_norm_stderr": 0.026564178111422622
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2569832402234637,
      "acc_stderr": 0.01461446582196633,
      "acc_norm": 0.2569832402234637,
      "acc_norm_stderr": 0.01461446582196633
    },
    "hendrycksTest-nutrition": {
      "acc": 0.6078431372549019,
      "acc_stderr": 0.027956046165424516,
      "acc_norm": 0.6078431372549019,
      "acc_norm_stderr": 0.027956046165424516
    },
    "hendrycksTest-philosophy": {
      "acc": 0.6045016077170418,
      "acc_stderr": 0.027770918531427838,
      "acc_norm": 0.6045016077170418,
      "acc_norm_stderr": 0.027770918531427838
    },
    "hendrycksTest-prehistory": {
      "acc": 0.5802469135802469,
      "acc_stderr": 0.027460099557005135,
      "acc_norm": 0.5802469135802469,
      "acc_norm_stderr": 0.027460099557005135
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.36879432624113473,
      "acc_stderr": 0.028782227561347237,
      "acc_norm": 0.36879432624113473,
      "acc_norm_stderr": 0.028782227561347237
    },
    "hendrycksTest-professional_law": {
      "acc": 0.37222946544980445,
      "acc_stderr": 0.01234624129720437,
      "acc_norm": 0.37222946544980445,
      "acc_norm_stderr": 0.01234624129720437
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.45588235294117646,
      "acc_stderr": 0.030254372573976694,
      "acc_norm": 0.45588235294117646,
      "acc_norm_stderr": 0.030254372573976694
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.4918300653594771,
      "acc_stderr": 0.02022513434305727,
      "acc_norm": 0.4918300653594771,
      "acc_norm_stderr": 0.02022513434305727
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6181818181818182,
      "acc_stderr": 0.046534298079135075,
      "acc_norm": 0.6181818181818182,
      "acc_norm_stderr": 0.046534298079135075
    },
    "hendrycksTest-security_studies": {
      "acc": 0.6285714285714286,
      "acc_stderr": 0.030932858792789855,
      "acc_norm": 0.6285714285714286,
      "acc_norm_stderr": 0.030932858792789855
    },
    "hendrycksTest-sociology": {
      "acc": 0.7114427860696517,
      "acc_stderr": 0.03203841040213321,
      "acc_norm": 0.7114427860696517,
      "acc_norm_stderr": 0.03203841040213321
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.74,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.74,
      "acc_norm_stderr": 0.0440844002276808
    },
    "hendrycksTest-virology": {
      "acc": 0.463855421686747,
      "acc_stderr": 0.03882310850890594,
      "acc_norm": 0.463855421686747,
      "acc_norm_stderr": 0.03882310850890594
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7485380116959064,
      "acc_stderr": 0.033275044238468436,
      "acc_norm": 0.7485380116959064,
      "acc_norm_stderr": 0.033275044238468436
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/ubuntu/Llama-customize/model,tokenizer=/home/ubuntu/Llama-customize/model,use_accelerate=True,device_map_option=auto",
    "num_fewshot": 5,
    "batch_size": "auto",
    "batch_sizes": [
      2
    ],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}